{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receive and processe a stream of user clicks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka_2.11:1.6.1, pyspark-shell'\n",
    "import time\n",
    "import io\n",
    "\n",
    "# pyspark and kafka\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# avro\n",
    "import avro\n",
    "import avro.schema\n",
    "from avro.io import DatumWriter, DatumReader\n",
    "from avro.datafile import DataFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read schema and create the avro reader\n",
    "\n",
    "```json\n",
    "{\"namespace\": \"trivago.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"ClickLog\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"user_id\", \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": \"time\", \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": \"action\", \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": \"destination\", \"type\": [\"int\", \"null\"]},\n",
    "     {\"name\": \"hotel\", \"type\": [\"int\", \"null\"]}\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "* actions can be: \n",
    "   * 1 = search\n",
    "   * 2 = filter\n",
    "   * 3 = click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACTION_SEARCH = 1\n",
    "ACTION_FILTER = 2\n",
    "ACTION_CLICK = 3\n",
    "\n",
    "checkpointDirectory = \"data/ssc_checkpoint\"\n",
    "batchDuration = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read schema\n",
    "schema_path = \"data/click_log.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "reader = avro.io.DatumReader(schema)\n",
    "\n",
    "def decoder(item):\n",
    "    # read decoded binary\n",
    "    decoded = reader.read(avro.io.BinaryDecoder(io.BytesIO(item)))\n",
    "    \n",
    "    # return a rdd Row\n",
    "    return Row(user_id=decoded['user_id'],\n",
    "               time=decoded['time'],\n",
    "               action=decoded['action'],\n",
    "               destination=decoded['destination'],\n",
    "               hotel=decoded['hotel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the _clicklog_ kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a streaming context. Check if there is a checkpoint from a previous failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createStreamingContext():\n",
    "    conf = SparkConf().setAppName(\"Streaming Clicks\") \\\n",
    "                      .setMaster(\"local[2]\")\n",
    "    sc = SparkContext(conf=conf) \n",
    "    ssc = StreamingContext(sc, batchDuration)\n",
    "    ssc.checkpoint(checkpointDirectory)   # set checkpoint directory\n",
    "    return ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ssc = StreamingContext.getOrCreate(checkpointDirectory, createStreamingContext)\n",
    "ssc = createStreamingContext()\n",
    "kvs = KafkaUtils.createStream(ssc, \n",
    "                              \"127.0.0.1:2181\", \n",
    "                              \"spark-streaming-consumer\", \n",
    "                              {'clicklog': 1},\n",
    "                              valueDecoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top10(rdd):\n",
    "    print rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "click_rdd = kvs.map(lambda x: x[1])\n",
    "click_rdd.filter(lambda x: x.action == ACTION_SEARCH) \\\n",
    "         .map(lambda x: (x.destination, 1)) \\\n",
    "         .reduceByKeyAndWindow(lambda x,y: x + y, \n",
    "                               lambda x,y: x - y,\n",
    "                               20, 5) \\\n",
    "         .map(lambda (a, b): (b, a)) \\\n",
    "         .transform(lambda rdd: rdd.sortByKey(ascending=False)) \\\n",
    "         .map(lambda (b, a): {\"destination\": a, \"search_count\": b}) \\\n",
    "         .foreachRDD(print_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
