{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cassandra _clicklog_ keyspace and _userclicksperday_ table:\n",
    "\n",
    "```\n",
    "CREATE KEYSPACE clicklog\n",
    "WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "USE clicklog;\n",
    "CREATE TABLE userclicksperday(\n",
    "   date text,\n",
    "   user_id int,\n",
    "   time int,\n",
    "   hotel int,\n",
    "   PRIMARY KEY ((date), user_id, time)\n",
    "   );\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import click_receiver\n",
    "from click_receiver import ClickReceiver\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "import os\n",
    "import avro\n",
    "import avro.schema\n",
    "from datetime import date, timedelta\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka_2.11:1.6.1,com.datastax.spark:spark-cassandra-connector_2.11:1.6.0-M2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACTION_SEARCH = 1\n",
    "ACTION_FILTER = 2\n",
    "ACTION_CLICK = 3\n",
    "duration = 30\n",
    "\n",
    "clickreceiver = ClickReceiver(\"clicklog\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top10(rdd):\n",
    "    \"\"\"Print an array with the 10 first elements of a rdd\"\"\"\n",
    "    print rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top10(rdd):\n",
    "    \"\"\"Get the top10 searched locations for the last 10 minutes\"\"\"\n",
    "    window_length = 30\n",
    "    sliding_interval = duration\n",
    "    \n",
    "    rdd.filter(lambda x: x.action == ACTION_SEARCH) \\\n",
    "       .map(lambda x: (x.destination, 1)) \\\n",
    "       .reduceByKeyAndWindow(lambda x, y: x + y,\n",
    "                             lambda x, y: x - y,\n",
    "                             window_length, sliding_interval) \\\n",
    "       .map(lambda (a, b): (b, a)) \\\n",
    "       .transform(lambda rdd: rdd.sortByKey(ascending=False)) \\\n",
    "       .map(lambda (b, a): {\"destination\": a, \"search_count\": b}) \\\n",
    "       .foreachRDD(print_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_cassandra_with_date(rdd):\n",
    "    if not rdd.isEmpty(): \n",
    "        df = click_receiver.sqlContext.createDataFrame(rdd)\n",
    "        to_pattern = 'yyyy-MM-dd'\n",
    "        selected_df = df.select(from_unixtime(df.time, to_pattern).alias(\"date\"),\n",
    "                                df.user_id,\n",
    "                                df.time,\n",
    "                                df.hotel)\n",
    "        selected_df.write \\\n",
    "                   .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                   .mode('append')\\\n",
    "                   .options(table=\"userclicksperday\", keyspace=\"clicklog\")\\\n",
    "                   .save()\n",
    "                \n",
    "#         print selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_stream_to_cassandra(rdd):\n",
    "    \"\"\"Save the data stream to cassandra for click messages\"\"\"\n",
    "    rdd.filter(lambda x: x.action == ACTION_CLICK) \\\n",
    "       .foreachRDD(save_to_cassandra_with_date)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clickreceiver.setup([get_top10, save_stream_to_cassandra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clickreceiver.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clickreceiver.ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read data from cassandra\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 949),\n",
       " (1, 910),\n",
       " (2, 476),\n",
       " (3, 840),\n",
       " (4, 999),\n",
       " (5, 964),\n",
       " (6, 526),\n",
       " (7, 689),\n",
       " (8, 253),\n",
       " (9, 417),\n",
       " (10, 579),\n",
       " (11, 542),\n",
       " (12, 903),\n",
       " (13, 468),\n",
       " (14, 30),\n",
       " (15, 794),\n",
       " (16, 158),\n",
       " (17, 721),\n",
       " (18, 481),\n",
       " (19, 646),\n",
       " (20, 608),\n",
       " (21, 971),\n",
       " (22, 740),\n",
       " (23, 297),\n",
       " (24, 60),\n",
       " (25, 622),\n",
       " (26, 785),\n",
       " (27, 548),\n",
       " (28, 111),\n",
       " (29, 876),\n",
       " (30, 37)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cassandra = click_receiver.sqlContext.read \\\n",
    "                             .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "                             .options(table=\"userclicksperday\", keyspace=\"clicklog\") \\\n",
    "                             .load()\n",
    "            \n",
    "# Due to the cassandra/spark bug https://github.com/holdenk/spark-testing-base/issues/101 \n",
    "# its not possible to query by the rowkey using equals comparation\n",
    "# the solution to query by yesterday is to query by date higher than before yesterday \n",
    "# and lower than today\n",
    "before_yesterday_str = str(date.today() - timedelta(2))\n",
    "today_str = str(date.today())\n",
    "tomorrow_str = str(date.today() + timedelta(1))\n",
    "out = df_cassandra.where((df_cassandra.date > before_yesterday_str) & \n",
    "                         (df_cassandra.date < tomorrow_str)) \\\n",
    "                  .select(df_cassandra.user_id, df_cassandra.hotel) \\\n",
    "                  .dropDuplicates(['user_id', 'hotel']) \\\n",
    "                  .rdd.map(lambda x: (x.user_id, x.hotel)) \\\n",
    "                  .groupByKey() \\\n",
    "                  .mapValues(lambda x: list(x)[1] if len(list(x)) > 1 else None )\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
